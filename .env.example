# Example environment variables for llmdy

OPENAI_API_KEY=anything

# Wherever your ReaderLM model is hosted, you can set the base URL here.
OPENAI_BASE_URL=http://localhost:8080/v1

# The name of the readerlm model returned from /v1/models
READERLM_MODEL=models/readerlm-v2

# The strategy to use for caching. Options are 'redis', 'memory', 'none'
# If llmdy is deployed in a memory-restricted environment, you should not use 'memory'
CACHE_STRATEGY=memory

# How long to keep a generated markdown before it is considered stale and needs to be generated again. This field is in seconds.
CACHE_TTL=3600

# In the case of interruption during markdown generation, there are ways to continue an incomplete generation. Options are 'redis', 'disk', 'none'
# By setting this to 'none', if generation fails midway, you will have to restart the entire generation. It is recommended to set this to 'disk', or 
# if you are on a serverless environment, set this to 'redis'
RECOVERY_STRATEGY=disk

# The connection string to the Redis instance
REDIS_URI=redis://localhost:6379/0

# The whisper model to use for audio transcription
WHISPER_MODEL=distil-whisper-large-v3-en

# The base URL to use the whisper model from. Must be OpenAI compatible. If left blank, it will default to OPENAI_API_KEY
WHISPER_BASE_URL=http://localhost:8080/v1

# API key to use for the whisper model. If left blank, it will default to OPENAI_API_KEY
WHISPER_API_KEY=anything

# The large language model to use as an agent
AGENT_MODEL=meta/llama3.3-70b-instruct

# The base URL of the agent model. If left blank, it will default to OPENAI_API_KEY
AGENT_BASE_URL=http://localhost:8080/v1

# API key to use for the agent model. If left blank it will default to OPENAI_API_KEY
AGENT_API_KEY=anything